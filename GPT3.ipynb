{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "084cd65b",
   "metadata": {},
   "source": [
    "First lets install the correct packages for GPT3.  We are already in the conda environment from jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da380a",
   "metadata": {},
   "source": [
    "First lets install pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed85d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.2\n",
      "  latest version: 4.10.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c nvidia -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf283b7",
   "metadata": {},
   "source": [
    "Now lets install HuggingFace.  It makes using popular Tranformers MUCH easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf424c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.9.2\n",
      "  latest version: 4.10.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c huggingface transformers -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aba525",
   "metadata": {},
   "source": [
    "Lets import the needed packages now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31bfee86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AJALI\\OneDrive\\Documents\\GitHub\\Chatbot with Flask\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c37a4",
   "metadata": {},
   "source": [
    "Now lets get the model.  We can either run the 1.3 billion paramater model or the 2.7 billion parameter model. Lets do the 2.7B model, which is \"EleutherAI/gpt-neo-2.7B\".  The 1.3B model is \"EleutherAI/gpt-neo-1.3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e795fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d00f01",
   "metadata": {},
   "source": [
    "This model can be ran on a GPU, but does not have to be. The 2.7B model takes slightly less than 13 GB of Vram.  The 1.3B model takes slighly less than 7.5GB of Vram.  The model will be placed on the GPU if there is one and if there is enough Vram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5c2bc2",
   "metadata": {},
   "source": [
    "Lets install pynvml to take a look at how much VRAM we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bde9e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pynvml in c:\\users\\ajali\\onedrive\\documents\\github\\chatbot with flask\\env\\lib\\site-packages (11.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf1002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_vram = 0.0\n",
    "if torch.cuda.is_available():\n",
    "    from pynvml import *\n",
    "    nvmlInit()\n",
    "    h = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(h)\n",
    "    free_vram = info.free/1048576000\n",
    "    print(\"There is a GPU with \" + str(free_vram) + \"GB of free VRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9834d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"EleutherAI/gpt-neo-2.7B\" and free_vram>13.5:\n",
    "    use_cuda = True\n",
    "    model.to(\"cuda:0\")\n",
    "elif model_name == \"EleutherAI/gpt-neo-1.3B\" and free_vram>7.5:\n",
    "    use_cuda = True\n",
    "    model.to(\"cuda:0\")\n",
    "else:\n",
    "    use_cuda = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf440c",
   "metadata": {},
   "source": [
    "Now we need to load the tokenizer to prepare the input for GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd79a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5b4cbf",
   "metadata": {},
   "source": [
    "We are almost done. At this point we need to decide what prompt we need to decide what prompt we want the model to continue, as well a how long we want the generated output to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e70b4877",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = str(input(\"Please enter a prompt: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3361b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_length = int(input(\"How long should the generated output be? \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9306669f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fried egg. He had no way of knowing that there was no point in his asking his father what had happened. Even if they had discussed the matter, he had no idea that his father had simply told him again and again that things were going to be different from now on, that they would return to the old order of things.\\n\\n## 16\\n\\n## A LITTLE BLUE\\n\\nIn the last week, the family was becoming increasingly aware of the changes being forced upon them. That was the result of the first crisis, of course, and that was the reason they had moved to the flat. The second crisis was the death of a friend, a man who had been to work on the _Maastricht_ project and who never spoke to them again. Their reaction was the same as it ever was. They had to accept that they were victims of fate, and it was clear to them that their old life was over. They had been trapped by a situation'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_Chat_response(text):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "    if use_cuda:\n",
    "        input_ids = input_ids.cuda()\n",
    "    gen_tokens = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=200, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    return tokenizer.batch_decode(gen_tokens)[0]\n",
    "    \n",
    "get_Chat_response(\"fried egg\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af23dde",
   "metadata": {},
   "source": [
    "We now need to tokenize the input prompt to prepare it for use with the model.  If we are using a GPU we will put it on the GPU as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aed05ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "if use_cuda:\n",
    "    input_ids = input_ids.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a16a0ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "gen_tokens = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4298458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRITE POEM ABOUT SUNDAYS IN LA and all the fun on the way!\n",
      "\n",
      "\n",
      "\n",
      "“The greatest gift is life’s love, and the most precious is love’s gift to us.” ― Friedrich Nietzsche\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“What was the worst thing you ever did?”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“You took your life, you made the wrong choices.” ― Bill Hicks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“I do what I love because it is the only thing I do. The rest of my life serves as the proof for the pudding.” ― Henry Ford\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“Everything in life is a journey and the person who laughs at the sky is the one who has arrived.” ― Dalai Lama\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“Someday in my life, I will meet someone who is just a little bit like you or a little bit like me or just a little bit something in between.\n"
     ]
    }
   ],
   "source": [
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde1cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc7f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
